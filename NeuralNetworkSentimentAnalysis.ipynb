{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Implement a Neural Network for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a neural network that performs sentiment analysis for a binary classification problem. \n",
    "\n",
    "1. Load the book review data set.\n",
    "2. Create training and test datasets.\n",
    "3. Transform the training and test text data using a TF-IDF vectorizer. \n",
    "4. Construct a neural network\n",
    "5. Train the neural network.\n",
    "6. Compare the model's performance on the training data vs test data.\n",
    "7. Improve its generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Load the Data Set\n",
    "\n",
    "We will work with the book review data set that contains book reviews taken from Amazon.com reviews.\n",
    "\n",
    "You will be working with the file named \"bookReviews.csv\" that is located in a folder named \"data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/bookReviews.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Labeled Examples\n",
    "\n",
    "* Get the `Positive_Review` column from DataFrame `df` and assign it to the variable `y`. This will be our label.\n",
    "* Get the `Review` column from  DataFrame `df` and assign it to the variable `X`. This will be our feature. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Positive Review']\n",
    "X = df['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Labeled Examples into Training and Test Sets   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Implement TF-IDF Vectorizer to Transform Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, transform the features into numerical vectors using `TfidfVectorizer`. \n",
    "\n",
    "1. Create a `TfidfVectorizer` object and save it to the variable `tfidf_vectorizer`.\n",
    "\n",
    "2. Call `tfidf_vectorizer.fit()` to fit the vectorizer to the training data `X_train`.\n",
    "\n",
    "3. Call the `tfidf_vectorizer.transform()` method to use the fitted vectorizer to transform the training data `X_train`. Save the result to `X_train_tfidf`.\n",
    "\n",
    "4. Call the `tfidf_vectorizer.transform()` method to use the fitted vectorizer to transform the test data `X_test`. Save the result to `X_test_tfidf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a TfidfVectorizer object \n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "# 2. Fit the vectorizer to X_train\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "\n",
    "# 3. Using the fitted vectorizer, transform the training data \n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "\n",
    "# 4. Using the fitted vectorizer, transform the test data \n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Construct a Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.  Define Model Structure\n",
    "\n",
    "Next we will create our neural network structure. We will create an input layer, three hidden layers and an output layer:\n",
    "\n",
    "* <b>Input layer</b>: The input layer will have the input shape corresponding to the vocabulary size. \n",
    "* <b>Hidden layers</b>: We will create three hidden layers of widths (number of nodes) 64, 32, and 16. They will utilize the ReLu activation function. \n",
    "* <b>Output layer</b>: The output layer will have a width of 1. The output layer will utilize the sigmoid activation function. Since we are working with binary classification, we will be using the sigmoid activation function to map the output to a probability between 0.0 and 1.0. We can later set a threshold and assume that the prediction is class 1 if the probability is larger than or equal to our threshold, or class 0 if it is lower than our threshold.\n",
    "\n",
    "To construct the neural network model using Keras, we will do the following:\n",
    "* We will use the Keras `Sequential` class to group a stack of layers. This will be our neural network model object.\n",
    "* We will use the `Dense` class to create each layer. \n",
    "* We will add each layer to the neural network model object.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create model object\n",
    "nn_model = keras.Sequential()\n",
    "\n",
    "\n",
    "\n",
    "# 2. Create the input layer and add it to the model object: \n",
    "\n",
    "# Create input layer:\n",
    "input_layer = keras.layers.InputLayer(input_shape=(vocabulary_size,))\n",
    "\n",
    "# Add input_layer to the model object:\n",
    "nn_model.add(input_layer)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Create the first hidden layer and add it to the model object:\n",
    "\n",
    "# Create input layer:\n",
    "hidden_layer_1 = keras.layers.Dense(units = 64, activation='relu')\n",
    "\n",
    "# Add hidden_layer_1 to the model object:\n",
    "nn_model.add(hidden_layer_1)\n",
    "\n",
    "\n",
    "\n",
    "# 4. Create the second layer and add it to the model object:\n",
    "\n",
    "# Create input layer:\n",
    "hidden_layer_2 = keras.layers.Dense(units = 32, activation='relu')\n",
    "\n",
    "# Add hidden_layer_2 to the model object:\n",
    "nn_model.add(hidden_layer_2)\n",
    "\n",
    "\n",
    "\n",
    "# 5. Create the third layer and add it to the model object:\n",
    "\n",
    "# Create input layer:\n",
    "hidden_layer_3 = keras.layers.Dense(units = 16, activation='relu')\n",
    "\n",
    "# Add hidden_layer_3 to the model object:\n",
    "nn_model.add(hidden_layer_3)\n",
    "\n",
    "nn_model.add(keras.layers.Dropout(.25))\n",
    "\n",
    "# 6. Create the output layer and add it to the model object:\n",
    "\n",
    "# Create input layer:\n",
    "output_layer = keras.layers.Dense(units = 1, activation='sigmoid')\n",
    "\n",
    "# Add output_layer to the model object:\n",
    "nn_model.add(output_layer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print summary of neural network model structure\n",
    "nn_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Define the Optimization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = keras.optimizers.SGD(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Define the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Compile the Model\n",
    "\n",
    "Package the network architecture with the optimizer and the loss function using the `compile()` method.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(optimizer = sgd_optimizer, loss=loss_fn, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Fit the Model on the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgBarLoggerNEpochs(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, num_epochs: int, every_n: int = 50):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.every_n = every_n\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.every_n == 0:\n",
    "            s = 'Epoch [{}/ {}]'.format(epoch + 1, self.num_epochs)\n",
    "            logs_s = ['{}: {:.4f}'.format(k.capitalize(), v)\n",
    "                      for k, v in logs.items()]\n",
    "            s_list = [s] + logs_s\n",
    "            print(', '.join(s_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the neural network model to the vectorized training data.\n",
    "<b>Note</b>: This may take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "t0 = time.time() # start time\n",
    "history = nn_model.fit(X_train_tfidf.toarray(), y_train, epochs = num_epochs, verbose= 0, callbacks=[ProgBarLoggerNEpochs(num_epochs, every_n=num_epochs)], validation_split = 0.2)\n",
    "t1 = time.time() # stop time\n",
    "print('Number of epochs: ', (num_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Model's Performance Over Time\n",
    "\n",
    "The code above outputs both the training loss and accuracy and the validation loss and accuracy. Let us visualize the model's performance over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(range(1, num_epochs + 1), history.history['loss'], label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(range(1, num_epochs + 1), history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Improve the Model and Evaluate the Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just evaluated our model's performance on the training and validation data. Let's now evaluate its performance on our test data and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = nn_model.evaluate(X_test_tfidf.toarray(), y_test)\n",
    "\n",
    "\n",
    "print('Loss: ', str(loss) , 'Accuracy: ', str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prevent Overfitting and Improve Model's Performance\n",
    "\n",
    "Neural networks can be prone to overfitting. Notice that the training accuracy is 100% but the test accuracy is around 82%. This indicates that our model is overfitting; it will not perform as well to new, previously unseen data as it did during training. We want to have an accurate idea of how well our model will generalize. Our goal is to have our training and testing accuracy scores be as close as possible.\n",
    "\n",
    "While there are different techniques that can be used to prevent overfitting, for the purpose of this exercise, we will focus on two methods:\n",
    "\n",
    "1. Changing the number of epochs. Too many epochs can lead to overfitting of the training dataset, whereas too few epochs may result in underfitting.\n",
    "\n",
    "2. Adding dropout regularization. During training, the nodes of a particular layer may always become influenced only by the output of a particular node in the previous layer, causing overfitting. Dropout regularization is a technique that randomly drops a number of nodes in a neural network during training as a way to adding randomization and prevent nodes from becoming dependent on one another. Adding dropout regularization can reduce overfitting and also improve the performance of the model. \n",
    "\n",
    "<b>Task:</b> \n",
    "\n",
    "1. Tweak the variable `num_epochs` above and restart and rerun all of the cells above. Evaluate the performance of the model on the training data and the test data.\n",
    "\n",
    "2. Add Keras `Dropout` layers after one or all hidden layers. Add the following line of code after you add a hidden layer to your model object:  `nn_model.add(keras.layers.Dropout(.25))`. The parameter `.25` is the fraction of the nodes to drop. You can experiment with this value as well. Restart and rerun all of the cells above. Evaluate the performance of the model on the training data and the test data.\n",
    "\n",
    "\n",
    "<b>Analysis:</b> \n",
    "In the cell below, specify the different approaches you used to reduce overfitting and summarize which configuration led to the best generalization performance.\n",
    "\n",
    "Did changing the number of epochs prevent overfitting? Which value of `num_epochs` yielded the closest training and testing accuracy score? Recall that too few epochs can lead to underfitting (both poor training and test performance). Which value of `num_epochs` resulted in the best accuracy score when evaluating the test data?\n",
    "\n",
    "Did adding dropout layers prevent overfitting? How so? Did it also improve the accuracy score when evaluating the test data? How many dropout layers did you add and which fraction of nodes did you drop? \n",
    "\n",
    "Record your findings in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At very low epoch numbers, the loss is incredibly high with much more loss than there is accuracy. Starting around 25 epochs, the accuracy becomes high and the loss almost 0. Increasing the number of epochs to greater than 100, leads to very long computational times which may not be worth the small gain in accuracy.\n",
    "Effect of adding drop out layer : Adding a dropout layer resulted in slightly slower increase in accuracy, though the difference is mostly neglible.\n",
    "\n",
    "A number between 25 and 50 seems to be an optimal number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without dropout layer \n",
    "\n",
    "Epoch [1/ [1, 10, 25, 50, 100, 150]], Loss: 0.6933, Accuracy: 0.5024, Val_loss: 0.6930, Val_accuracy: 0.4905 Number of epochs: 1\n",
    "\n",
    "Epoch [10/ [1, 10, 25, 50, 100, 150]], Loss: 0.6070, Accuracy: 0.6537, Val_loss: 0.5790, Val_accuracy: 0.6741 Number of epochs: 10\n",
    "\n",
    "Epoch [25/ [1, 10, 25, 50, 100, 150]], Loss: 0.0036, Accuracy: 1.0000, Val_loss: 0.5009, Val_accuracy: 0.8196 Number of epochs: 25\n",
    "\n",
    "Epoch [50/ [1, 10, 25, 50, 100, 150]], Loss: 0.0004, Accuracy: 1.0000, Val_loss: 0.6112, Val_accuracy: 0.8133 Number of epochs: 50\n",
    "\n",
    "Epoch [100/ [1, 10, 25, 50, 100, 150]], Loss: 0.0001, Accuracy: 1.0000, Val_loss: 0.6775, Val_accuracy: 0.8196 Number of epochs: 100\n",
    "\n",
    "Epoch [150/ [1, 10, 25, 50, 100, 150]], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.7226, Val_accuracy: 0.8165 Number of epochs: 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding dropout layer\n",
    "\n",
    "Epoch [1/ [1, 10, 25, 50, 100, 150]], Loss: 0.6933, Accuracy: 0.5071, Val_loss: 0.6930, Val_accuracy: 0.5095\n",
    "Number of epochs:  1\n",
    "\n",
    "Epoch [10/ [1, 10, 25, 50, 100, 150]], Loss: 0.5807, Accuracy: 0.7425, Val_loss: 0.5873, Val_accuracy: 0.7753\n",
    "Number of epochs:  10\n",
    "\n",
    "Epoch [25/ [1, 10, 25, 50, 100, 150]], Loss: 0.0196, Accuracy: 1.0000, Val_loss: 0.5037, Val_accuracy: 0.8133\n",
    "Number of epochs:  25\n",
    "\n",
    "Epoch [50/ [1, 10, 25, 50, 100, 150]], Loss: 0.0026, Accuracy: 1.0000, Val_loss: 0.7512, Val_accuracy: 0.8038\n",
    "Number of epochs:  50\n",
    "\n",
    "Epoch [100/ [1, 10, 25, 50, 100, 150]], Loss: 0.0010, Accuracy: 1.0000, Val_loss: 0.9175, Val_accuracy: 0.8038\n",
    "Number of epochs:  100\n",
    "\n",
    "Epoch [150/ [1, 10, 25, 50, 100, 150]], Loss: 0.0002, Accuracy: 1.0000, Val_loss: 1.0891, Val_accuracy: 0.8038\n",
    "Number of epochs:  150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_predictions = nn_model.predict(X_test_tfidf.toarray())\n",
    "\n",
    "print(\"Predictions for the first 10 examples:\")\n",
    "print(\"Probability\\t\\t\\tClass\")\n",
    "for i in range(0,10):\n",
    "    if probability_predictions[i] >= .5:\n",
    "        class_pred = \"Good Review\"\n",
    "    else:\n",
    "        class_pred = \"Bad Review\"\n",
    "    print(str(probability_predictions[i]) + \"\\t\\t\\t\" + str(class_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Review #1:\\n')\n",
    "print(X_test.to_numpy()[56])\n",
    "\n",
    "goodReview = True if probability_predictions[56] >= .5 else False\n",
    "    \n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(goodReview))\n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[56]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Review #2:\\n')\n",
    "print(X_test.to_numpy()[24])\n",
    "\n",
    "goodReview = True if probability_predictions[24] >= .5 else False\n",
    "\n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(goodReview)) \n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[24]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
